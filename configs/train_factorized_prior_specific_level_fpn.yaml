# Specific Level FPN Feature Compression Configuration
# This config trains one FactorizedPrior model on a single, specified FPN level
# The level is specified via command line argument --fpn_level

model:
  name: "factorized_prior_specific_level"
  n_hidden: 128  # Number of hidden channels in analysis/synthesis transforms  
  n_channels: 192  # Number of channels in latent representation
  fpn_channels_per_level: 256  # Number of channels per FPN level (standard for ResNet50 FPN)

# Data configuration
data:
  train_list: "data/processed/kitti/train.txt"
  val_list: "data/processed/kitti/val.txt"
  test_list: "data/processed/kitti/test.txt"
  
  # Transform configuration for input images (before FPN extraction)
  transforms:
    resize:
      enabled: true
      size: [384, 1280]  # Standard size for FPN extraction
    
    crop:
      enabled: false  # No cropping - use full images for FPN
      size: 256
    
    flip:
      enabled: false  # Disable augmentation for more stable FPN features
      probability: 0.5
    
    color_jitter:
      enabled: false  # Disabled for compression tasks
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
      
    normalize:
      enabled: true
      mean: [0.485, 0.456, 0.406]  # ImageNet normalization for detection model
      std: [0.229, 0.224, 0.225]
  
  # Transform configuration for testing/validation
  test_transforms:
    resize:
      enabled: true
      size: [384, 1280]  # Same as training for consistency
    
    crop:
      enabled: false
      size: 256
    
    flip:
      enabled: false
      probability: 0.5
    
    color_jitter:
      enabled: false
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
      
    normalize:
      enabled: true
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

# Training configuration
training:
  batch_size: 8  # Smaller batch size due to FPN feature extraction overhead
  epochs: 100  # Can train longer since focusing on one level
  learning_rate: 1e-4  # Lower learning rate for fine-tuning on features
  lambda: 0.01  # Rate-distortion trade-off parameter
                 # Options: 0.0001, 0.0005, 0.001, 0.005, 0.01
  num_workers: 4
  log_interval: 10  # Log more frequently for monitoring
  save_dir: "checkpoints/fpn_specific_level_compression"  # Will be modified to include level
  
  # Optimization settings
  optimizer:
    type: "Adam"
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 1e-4  # Small weight decay for regularization
  
  # Learning rate schedule
  lr_schedule:
    enabled: true
    type: "plateau"
    patience: 8  # Can be more patient since training on one level
    factor: 0.8  # Smaller reduction factor
    min_lr: 1e-6
    
  # Early stopping
  early_stopping:
    enabled: true
    patience: 20  # More patience for single level training
    min_delta: 1e-6

# Validation settings
validation:
  interval: 5  # Validate every 5 epochs
  save_reconstructions: true  # Save feature analysis
  num_samples: 4  # Number of samples to analyze

# Experiment configuration
experiment:
  name: "fpn_specific_level_compression"
  description: "Training single FactorizedPrior model on a specific FPN level. The level is specified via --fpn_level command line argument. This allows training specialized models for each level to study level-specific compression characteristics."
  notes: |
    - Single model trained on one specific FPN level only
    - Level specified via command line argument (0-4)
    - Input/output: 256 channels (FPN feature channels)
    - Model specialized for level-specific characteristics
    - Checkpoints saved with level identifier for easy distinction
    - Useful for comparing compression performance across levels
    - Can train multiple models in parallel for different levels