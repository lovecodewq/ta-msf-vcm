# Configuration for training JointAutoregressiveHierarchicalPriors model on FPN features
# with random level sampling

model:
  # Model architecture parameters
  n_hidden: 192     # Number of hidden channels (M in the paper, typically 2-3x n_latent)
  n_latent: 192    # Number of channels in latent space (N in the paper)
  fpn_channels_per_level: 256  # FPN features have 256 channels per level

data:
  train_list: "data/processed/kitti/train.txt"
  val_list: "data/processed/kitti/val.txt"
  test_list: "data/processed/kitti/test.txt"
  
  # Transform configuration for input images (before FPN extraction)
  transforms:
    resize:
      enabled: true
      size: [384, 1280]  # Standard size for FPN extraction
    
    crop:
      enabled: false  # No cropping - use full images for FPN
      size: 256
    
    flip:
      enabled: false  # Disable augmentation for more stable FPN features
      probability: 0.5
    
    color_jitter:
      enabled: false  # Disabled for compression tasks
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
      
    normalize:
      enabled: true
      mean: [0.485, 0.456, 0.406]  # ImageNet normalization for detection model
      std: [0.229, 0.224, 0.225]
  
  # Transform configuration for testing/validation
  test_transforms:
    resize:
      enabled: true
      size: [384, 1280]  # Same as training for consistency
    
    crop:
      enabled: false
      size: 256
    
    flip:
      enabled: false
      probability: 0.5
    
    color_jitter:
      enabled: false
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
      
    normalize:
      enabled: true
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

training:
  # Basic training parameters
  batch_size: 8
  epochs: 100
  learning_rate: 1.0e-4
  lambda: 50  # Rate-distortion trade-off parameter
  num_workers: 4
  log_interval: 10  # Log every N batches
  save_dir: "checkpoints/fpn_joint_auto_compression_fused_feature"

  # Optimizer configuration (Adam)
  optimizer:
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 1.0e-4

  # Learning rate schedule
  lr_schedule:
    enabled: true
    factor: 0.5      # Multiply LR by this factor on plateau
    patience: 5      # Number of epochs to wait before reducing LR
    min_lr: 1.0e-6   # Don't reduce LR below this value

  # Early stopping
  early_stopping:
    enabled: true
    patience: 5     # Number of epochs to wait before stopping
    min_delta: 1.0e-4  # Minimum change in validation loss to qualify as an improvement

validation:
  interval: 5          # Run validation every N epochs
  num_samples: 4       # Number of samples to use for detailed analysis
  save_reconstructions: true  # Save reconstructed features for analysis

# Compression specific parameters
compression:
  # Entropy bottleneck parameters
  entropy_bottleneck:
    init_scale: 10  # Initial scale for the entropy bottleneck
    filters: 3      # Number of filters in entropy parameters MLPs
    likelihood_bound: 1e-9  # Bound on likelihood values for numerical stability
  
  # Gaussian Conditional parameters
  gaussian_conditional:
    scale_table: null  # Use learned scales instead of predefined table
    scale_bound: 0.11  # Minimum scale value
    tail_mass: 1e-9    # Mass of the distribution tails to preserve
  
  # Context model parameters
  context_model:
    masked_conv_kernel_size: 5  # Kernel size for masked convolutions
    causal_context: true       # Use causal context only